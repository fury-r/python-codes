{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UUx5Li7OcBh"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yRRsL279OjaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "YKB9R_IdPJYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path_to_file)\n",
        "text=open(path_to_file,'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q39mSsr9PNfF",
        "outputId": "fd63e836-206e-40b2-a11c-4ffcbe88b5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.keras/datasets/shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCm51UTPaHz",
        "outputId": "bc5af181-1f7a-48b7-d32c-77025de541b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=sorted(set(text))"
      ],
      "metadata": {
        "id": "ktgRkpxQTb7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx={u:i for i,u in enumerate(vocab)}\n",
        "idx2char=np.array(vocab)\n",
        "print(idx2char)\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CzCZ32PTewi",
        "outputId": "58251a1a-ba10-4922-a4c1-6a51f5d521cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int=text_to_int(text)"
      ],
      "metadata": {
        "id": "9meFBZAsVU-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:13])\n",
        "print(text_as_int[:13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcHyuvSSVY98",
        "outputId": "363e6255-3340-4eb0-aa00-a7eaa787440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints=ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O6cbQH8Vpin",
        "outputId": "8ec6cd3b-b01b-45ef-a08f-a1c32a169515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100\n",
        "examples_per_epoch=len(text)//(seq_length+1)\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "E_u_xTlrXeTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=char_dataset.batch(seq_length+1,drop_remainder=True)"
      ],
      "metadata": {
        "id": "aTO7wjSDX8k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text=chunk[:-1] #hell\n",
        "  target_text=chunk[1:] #ello\n",
        "  return input_text,target_text\n",
        "dataset=sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "XlaUHvK9YG3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print(f\"example\\ninput:{int_to_text(x)}\\noutput:{int_to_text(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nON9NcdaYZm7",
        "outputId": "758c264a-9975-413b-e135-fdcc75933248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example\n",
            "input:First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "output:irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "example\n",
            "input:are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "output:re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "vocab_size=len(vocab)\n",
        "embedding_dim=256\n",
        "rnn_units=1024\n",
        "buffer_size=10000\n",
        "data=dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
      ],
      "metadata": {
        "id": "bt-HPGefZEMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
        "  model=tf.keras.Sequential(\n",
        "      [\n",
        "       tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "       tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "       tf.keras.layers.Dense(vocab_size)\n",
        "      ]\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "tcBQgATEZsGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(vocab_size,embedding_dim,rnn_units,batch_size)"
      ],
      "metadata": {
        "id": "WfUk779hbCNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sZe_NMkbPCd",
        "outputId": "3a998ca8-4a77-431f-b741-7a6643ba5c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch,target_example_batch in data.take(1):\n",
        "  example_batch_predictions=model(input_example_batch)\n",
        "  print(example_batch_predictions.shape) #(batch_size,sequence_length,vocab_size)"
      ],
      "metadata": {
        "id": "5QtDGy7TbSdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c263c4e0-9ae0-4384-8f4a-fb2d8cf286ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "(64, 100, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(example_batch_predictions))\n",
        "#print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "6JlawmUtgD-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29a1302-ca2d-4e07-994e-47b1e249ca7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "time_pred=pred[0]\n",
        "print(len(time_pred))\n"
      ],
      "metadata": {
        "id": "cO-snJX-gTyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0f21d2-040c-48ec-8fbf-c2b359f96dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices=tf.random.categorical(pred,num_samples=1)\n",
        "sampled_indices=np.reshape(sampled_indices,(1,-1))[0]\n",
        "predicted_chars=int_to_text(sampled_indices)\n",
        "predicted_chars"
      ],
      "metadata": {
        "id": "N9sC_m4Bgg7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad7f0b7e-e2e6-4290-a55d-94ba07b8bd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"iPUmEks\\n.rUGPRPz.Mb.\\ncNFoIjaOYhj$?.$'p -afaErHRiEDALavBU'S Xm xaG3DiauUDckhEtxMVWJubppVL,UIr-iXYXoJ \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function\n",
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
      ],
      "metadata": {
        "id": "eRjWiJCWhZ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "cJSiJyXJhkC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir='./training_checkpoints'\n",
        "checkpoint_prefix=os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "ug4zAxGEiBP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(data,epochs=50,callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uLOXE8viiRD",
        "outputId": "2db008df-a0ba-4768-e589-ec35208f75e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 2.6639\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.9574\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.6962\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.5548\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.4699\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.4122\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.3677\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.3306\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.2990\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.2694\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.2410\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.2136\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.1838\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.1547\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.1242\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.0916\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.0589\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.0246\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.9889\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.9542\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.9182\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.8815\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.8478\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.8132\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.7813\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.7511\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.7221\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.6955\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6698\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.6450\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6259\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.6057\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5878\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.5724\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5571\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.5428\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5324\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5195\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5099\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4996\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4910\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4828\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.4735\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4680\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4634\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4549\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4505\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4451\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4402\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(vocab_size,embedding_dim,rnn_units,batch_size=1)"
      ],
      "metadata": {
        "id": "cyCXtRaYmhtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num=10\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "qXBaQVUrm1dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,start_string):\n",
        "  num_generate=800\n",
        "\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "\n",
        "  input_eval=tf.expand_dims(input_eval,0)\n",
        "\n",
        "  text_generated=[]\n",
        "\n",
        "  temperature=1.0\n",
        "\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "    predictions=model(input_eval)\n",
        "\n",
        "    predictions=tf.squeeze(predictions,0)\n",
        "\n",
        "    predictions=predictions/temperature\n",
        "\n",
        "    predicted_id=tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval=tf.expand_dims([predicted_id],0)\n",
        "\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string+\"\".join(text_generated))"
      ],
      "metadata": {
        "id": "jvLfCrHvnLl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=input(\"Type a starting string: \")\n",
        "print(generate_text(model,inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa65W2KSoRMh",
        "outputId": "e53bee06-61b6-439d-b2ae-afd8b3d4c353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeous heast is all of fury: shot to thy love:\n",
            "and then we cannot, sir, that I am now gone;\n",
            "Go, rall my hounds shall know.\n",
            "\n",
            "KING RICHARD II:\n",
            "No matter where is my fellow Tranio.\n",
            "\n",
            "TRANIO:\n",
            "Here comes no meal.\n",
            "Good lady, good queen; I say, I'll rison for you:\n",
            "Look, and the commanded morning lives of death,\n",
            "But be at glim quit their own report.\n",
            "\n",
            "Servant:\n",
            "These detes mistrust its like the harm, time paul,\n",
            "Thou canst demand was said is now he did\n",
            "temper and tear of their sixteency, and fune more matter;\n",
            "Prodigrace for your vantage. Be relegs and reason could I effected\n",
            "To know our duty, and together with the heart.\n",
            "Take thy commission.\n",
            "\n",
            "ANGELO:\n",
            "Now, good my lord,\n",
            "Like almost time to do some four contented:\n",
            "Methinks yourself no hatred, though not in prison.\n",
            "\n",
            "ISABELLA:\n",
            "O, I do fear thee, Tranio, be co\n"
          ]
        }
      ]
    }
  ]
}